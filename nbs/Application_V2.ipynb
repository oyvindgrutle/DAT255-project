{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Application\n",
    "\n",
    "This notebook is created as an application for using the trained model on new audio and give a classification based on the 41 different classes:\n",
    "\n",
    " > 'Hi-hat' 'Saxophone' 'Trumpet' 'Glockenspiel' 'Cello' 'Knock'\n",
    " 'Gunshot_or_gunfire' 'Clarinet' 'Computer_keyboard' 'Keys_jangling'\n",
    " 'Snare_drum' 'Writing' 'Laughter' 'Tearing' 'Fart' 'Oboe' 'Flute' 'Cough'\n",
    " 'Telephone' 'Bark' 'Chime' 'Bass_drum' 'Bus' 'Squeak' 'Scissors'\n",
    " 'Harmonica' 'Gong' 'Microwave_oven' 'Burping_or_eructation' 'Double_bass'\n",
    " 'Shatter' 'Fireworks' 'Tambourine' 'Cowbell' 'Electric_piano' 'Meow'\n",
    " 'Drawer_open_or_close' 'Applause' 'Acoustic_guitar' 'Violin_or_fiddle'\n",
    " 'Finger_snapping'\n",
    " \n",
    "The application is built using ipywidgets and voilà. Ipywidgets makes it easy to make buttons and display the audio and results. Voilà removes everything but the widgets so that it looks clean and all code is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "#import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress warnings that appear when uploading and classifying audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add recorder inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install ffmpeg\n",
    "# !pip install torchaudio ipywebrtc\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths to where the recordings are being saved\n",
    "data_path = Path('../../data/')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "audio_path = data_path/'audio'\n",
    "audio_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the recorder to put in a widget later\n",
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path is just set to be the path used in the github repo. Change it to where you have the model located.\n",
    "```\n",
    "├───models\n",
    "│   └─── model_V2.pkl\n",
    "└───nbs\n",
    "    ├─── Application.ipynb\n",
    "    └───.ipynb_checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../')\n",
    "model_path = path/'models'\n",
    "\n",
    "try:\n",
    "    learn_inf = load_learner(model_path/'model_V2.pkl')\n",
    "except:\n",
    "    posix_backup = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    learn_inf = load_learner(model_path/'model_V2.pkl')\n",
    "    pathlib.PosixPath = posix_backup\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload(accept=\".wav,.mp3\")\n",
    "out_rec = widgets.Output()\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "lbl_pred.value = 'Please select audio'\n",
    "btn_run = widgets.Button(description='Classify recorded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rec.clear_output()\n",
    "with out_rec: display(recorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting audio to images\n",
    "\n",
    "This is the same function for converting audio to mel spectrograms as used in training the model, except for a few tweaks to make it work with the audio file from the uploader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mel_spec_tfm(audio, dst_path = path/'../data/imgs/uploaded'):\n",
    "    data, sample_rate = audio\n",
    "    \n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    n_mels = 80\n",
    "    fmin = 20\n",
    "    fmax = sample_rate / 2 \n",
    "    \n",
    "    mel_spec_power = librosa.feature.melspectrogram(data, sr=sample_rate, n_fft=n_fft, \n",
    "                                                    hop_length=hop_length, \n",
    "                                                    n_mels=n_mels, power=2.0, \n",
    "                                                    fmin=fmin, fmax=fmax)\n",
    "    \n",
    "    #mel_spec_power = librosa.feature.melspectrogram(x, sr=sample_rate)\n",
    "    \n",
    "    mel_spec_db = librosa.power_to_db(mel_spec_power, ref=np.max)\n",
    "    \n",
    "    \n",
    "    dst_path.mkdir(exist_ok=True)\n",
    "    try:\n",
    "        fname = list(btn_upload.value)[0]\n",
    "    except:\n",
    "        fname = 'file.wav'\n",
    "    \n",
    "    plt.imsave(dst_path / (fname[:-4] + '.png'), mel_spec_db)\n",
    "    \n",
    "    return dst_path / (fname[:-4] + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model on uploaded data\n",
    "\n",
    "When an audiofile is uploaded this function will run which makes a prediction on the image representation of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click(change):\n",
    "    out_pl.clear_output()\n",
    "    with out_pl: display(Audio(btn_upload.data[-1]))\n",
    "    dst_path = path/'../data/imgs/uploaded'\n",
    "    audio = librosa.load(io.BytesIO(btn_upload.data[-1]))\n",
    "    audio_img = log_mel_spec_tfm(audio, dst_path)\n",
    "    pred,pred_idx,probs = learn_inf.predict(audio_img)\n",
    "    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on_click_live is almost the same as on_click. This function is called when clicking the clasify button and classifies the sound recording that has been recording using the built in recorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_live(change):\n",
    "    out_pl.clear_output()\n",
    "    update_live_audio()\n",
    "    audio = librosa.load(audio_path/'file.wav')\n",
    "    x, sr = audio\n",
    "    with out_pl: display(Audio(data=x, rate=sr))\n",
    "    dst_path = path/'../data/imgs/uploaded'\n",
    "    audio_img = log_mel_spec_tfm(audio, dst_path)\n",
    "    pred,pred_idx,probs = learn_inf.predict(audio_img)\n",
    "    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the audio file from the recorder as wav and puts it in a player\n",
    "def update_live_audio():\n",
    "    with open(audio_path/'recording.webm', 'wb') as f:\n",
    "        f.write(recorder.audio.value)\n",
    "    !ffmpeg -i ../../data/audio/recording.webm -ac 1 -f wav ../../data/audio/file.wav -y -hide_banner -loglevel panic\n",
    "    sig, sr = librosa.load(audio_path/'file.wav')\n",
    "    print(sig.shape)\n",
    "    Audio(data=sig, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_click, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_run.on_click(on_click_live)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VBox gathers all the widgets specified and is what makes it look nice and tidy.\n",
    "\n",
    "If you upload multiple audio files, it will only use the last file uploaded to create a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0635c75baf429688600b4524f73aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select your audio by using \"upload\" or record by tapping the record button followe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63504,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-119-6a112aefa9d5>:10: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.01548612 -0.01407828\n",
      " -0.01415384] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_spec_power = librosa.feature.melspectrogram(data, sr=sample_rate, n_fft=n_fft,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-119-6a112aefa9d5>:10: FutureWarning: Pass y=[-4.13678299e-06  7.62852596e-06 -3.93728942e-06 ...  2.04747936e-04\n",
      "  1.01241698e-04  1.01913436e-04] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_spec_power = librosa.feature.melspectrogram(data, sr=sample_rate, n_fft=n_fft,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.VBox([widgets.Label('Select your audio by using \"upload\" or record by tapping the record button followed by \"Classife recorded\"'),\n",
    "      out_rec, btn_run, btn_upload, out_pl, lbl_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install voila\n",
    "#!jupyter serverextension enable voila --sys-prefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
