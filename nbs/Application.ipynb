{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Application\n",
    "\n",
    "This notebook is created as an application for using the trained model on new audio and give a classification based on the 41 different classes:\n",
    "\n",
    " > 'Hi-hat' 'Saxophone' 'Trumpet' 'Glockenspiel' 'Cello' 'Knock'\n",
    " 'Gunshot_or_gunfire' 'Clarinet' 'Computer_keyboard' 'Keys_jangling'\n",
    " 'Snare_drum' 'Writing' 'Laughter' 'Tearing' 'Fart' 'Oboe' 'Flute' 'Cough'\n",
    " 'Telephone' 'Bark' 'Chime' 'Bass_drum' 'Bus' 'Squeak' 'Scissors'\n",
    " 'Harmonica' 'Gong' 'Microwave_oven' 'Burping_or_eructation' 'Double_bass'\n",
    " 'Shatter' 'Fireworks' 'Tambourine' 'Cowbell' 'Electric_piano' 'Meow'\n",
    " 'Drawer_open_or_close' 'Applause' 'Acoustic_guitar' 'Violin_or_fiddle'\n",
    " 'Finger_snapping'\n",
    " \n",
    "The application is built using ipywidgets and voilà. Ipywidgets makes it easy to make buttons and display the audio and results. Voilà removes everything but the widgets so that it looks clean and all code is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress warnings that appear when uploading and classifying audio.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path is just set to be the path used in the github repo. Change it to where you have the model located.\n",
    "```\n",
    "├───models\n",
    "│   └─── model_V2.pkl\n",
    "└───nbs\n",
    "    ├─── Application.ipynb\n",
    "    └───.ipynb_checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../')\n",
    "model_path = path/'models'\n",
    "\n",
    "# This try/catch fixes an error on windows when using PosixPath\n",
    "try:\n",
    "    learn_inf = load_learner(model_path/'model_V2.pkl')\n",
    "except:\n",
    "    posix_backup = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    learn_inf = load_learner(model_path/'model_V2.pkl')\n",
    "    pathlib.PosixPath = posix_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the the widgets for uploading audio, \n",
    "# playing audio and displaying the prediction.\n",
    "btn_upload = widgets.FileUpload(accept=\".wav,.mp3\")\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "lbl_pred.value = 'Please select audio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting audio to images\n",
    "\n",
    "This is the same function for converting audio to mel spectrograms as used in training the model, except for a few tweaks to make it work with the audio file from the uploader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mel_spec_tfm(audio_upload, dst_path = path/'../data/imgs/uploaded'):\n",
    "    data, sample_rate = librosa.load(io.BytesIO(audio_upload.data[-1]))\n",
    "    \n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    n_mels = 80\n",
    "    fmin = 20\n",
    "    fmax = sample_rate / 2 \n",
    "    \n",
    "    mel_spec_power = librosa.feature.melspectrogram(data, sr=sample_rate, n_fft=n_fft, \n",
    "                                                    hop_length=hop_length, \n",
    "                                                    n_mels=n_mels, power=2.0, \n",
    "                                                    fmin=fmin, fmax=fmax)\n",
    "    \n",
    "    #mel_spec_power = librosa.feature.melspectrogram(x, sr=sample_rate)\n",
    "    \n",
    "    mel_spec_db = librosa.power_to_db(mel_spec_power, ref=np.max)\n",
    "    \n",
    "    \n",
    "    dst_path.mkdir(exist_ok=True)\n",
    "    fname = list(audio_upload.value)[0]\n",
    "    \n",
    "    plt.imsave(dst_path / (fname[:-4] + '.png'), mel_spec_db)\n",
    "    \n",
    "    return dst_path / (fname[:-4] + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model on uploaded data\n",
    "\n",
    "When an audiofile is uploaded this function will run which makes a prediction on the image representation of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dst_path to where you want the spectrogram \n",
    "# of your uploaded audio to be saved.\n",
    "def on_click_classify(change):\n",
    "    dst_path = path/'../data/imgs/uploaded'\n",
    "    out_pl.clear_output()\n",
    "    with out_pl: display(Audio(btn_upload.data[-1]))\n",
    "    audio_img = log_mel_spec_tfm(btn_upload, dst_path)\n",
    "    pred,pred_idx,probs = learn_inf.predict(audio_img)\n",
    "    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_click_classify, names=['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VBox gathers all the widgets specified and is what makes it look nice and tidy.\n",
    "\n",
    "If you upload multiple audio files, it will only use the last file uploaded to create a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c3ea89c4284b8babb70ec1f227d0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select your audio'), FileUpload(value={}, accept='.wav,.mp3', description='Upload'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.VBox([widgets.Label('Select your audio'),\n",
    "      btn_upload, out_pl, lbl_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install voila\n",
    "#!jupyter serverextension enable voila --sys-prefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
